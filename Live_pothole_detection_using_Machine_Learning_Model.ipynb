{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwpja-Up3TU6"
      },
      "source": [
        "# **Live pothole detection using Machine Learning Model.**\n",
        "**Done by**\n",
        "Shivsharan Patil and Team, Presidency University\n",
        "\n",
        "This project essentially converts jhasuman's neural network based desktop pothole detector above (fp32 aka single precision floating point/32 bits), to jetson nano neural network based pothole detector (fp16 half precision floating point 16 bits). (Purpose of which is to add the jetson nano with the trained half precision pothole detector to my car)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma8JcJc9pzmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f350c31b-dd6f-4f7e-97f3-3ae13b6f96d0"
      },
      "source": [
        "\n",
        "###################\n",
        "#Part A/0. Connect to Google drive to access saved neural nwtwork wrights etc\n",
        "###################\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w5JdZS6Nm0VP",
        "outputId": "dd3e735b-23ce-40f4-ce91-a66f11a3d104"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.0.0a0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.5.1-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 9.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 165 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.5.0)\n",
            "Installing collected packages: numpy, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.16.2\n",
            "    Uninstalling numpy-1.16.2:\n",
            "      Successfully uninstalled numpy-1.16.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.0.0a0\n",
            "    Uninstalling tensorflow-2.0.0a0:\n",
            "      Successfully uninstalled tensorflow-2.0.0a0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.5 tensorflow-2.5.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow_gpu-2.5.1-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 10 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.34.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (57.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.32.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.5.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.5.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWBtb7Xin-zG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b4741b-6ff2-464f-db06-97f0201d5b46"
      },
      "source": [
        "\n",
        "###################\n",
        "#Part A/1. backend.py from https://github.com/jhasuman/potholes-detection\n",
        "###################\n",
        "\n",
        "jordan_drive_full_yolo_backend_id = '1LjcG4pA8mdF0UidPLH2Ov-qiATT0Map8';\n",
        "file_obj_full_yolo_backend = drive.CreateFile({'id': jordan_drive_full_yolo_backend_id})\n",
        "file_obj_full_yolo_backend.GetContentFile ( \"full_yolo_backend.h5\" )\n",
        "\n",
        "from keras import applications\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "basemodel = InceptionV3(weights='imagenet', include_top=False)\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "FULL_YOLO_BACKEND_PATH  = \"full_yolo_backend.h5\"   # should be hosted on a server\n",
        "TINY_YOLO_BACKEND_PATH  = \"tiny_yolo_backend.h5\"   # should be hosted on a server\n",
        "SQUEEZENET_BACKEND_PATH = \"squeezenet_backend.h5\"  # should be hosted on a server\n",
        "MOBILENET_BACKEND_PATH  = \"mobilenet_backend.h5\"   # should be hosted on a server\n",
        "INCEPTION3_BACKEND_PATH = \"inception_backend.h5\"   # should be hosted on a server\n",
        "VGG16_BACKEND_PATH      = \"vgg16_backend.h5\"       # should be hosted on a server\n",
        "RESNET50_BACKEND_PATH   = \"resnet50_backend.h5\"    # should be hosted on a server\n",
        "\n",
        "class BaseFeatureExtractor(object):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "\n",
        "    # to be defined in each subclass\n",
        "    def __init__(self, input_size):\n",
        "        raise NotImplementedError(\"error message\")\n",
        "\n",
        "    # to be defined in each subclass\n",
        "    def normalize(self, image):\n",
        "        raise NotImplementedError(\"error message\")\n",
        "\n",
        "    def get_output_shape(self):\n",
        "        return self.feature_extractor.get_output_shape_at(-1)[1:3]\n",
        "\n",
        "    def extract(self, input_image):\n",
        "        return self.feature_extractor(input_image)\n",
        "\n",
        "class FullYoloFeature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        input_image = Input(shape=(input_size, input_size, 3))\n",
        "\n",
        "        # the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\n",
        "        def space_to_depth_x2(x):\n",
        "            return tf.space_to_depth(x, block_size=2)\n",
        "\n",
        "        # Layer 1\n",
        "        x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
        "        x = BatchNormalization(name='norm_1')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 2\n",
        "        x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_2')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 3\n",
        "        x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_3')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 4\n",
        "        x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_4')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 5\n",
        "        x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_5')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 6\n",
        "        x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_6')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 7\n",
        "        x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_7')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 8\n",
        "        x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_8')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 9\n",
        "        x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_9')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 10\n",
        "        x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_10')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 11\n",
        "        x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_11')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 12\n",
        "        x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_12')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 13\n",
        "        x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_13')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        skip_connection = x\n",
        "\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 14\n",
        "        x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_14')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 15\n",
        "        x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_15')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 16\n",
        "        x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_16')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 17\n",
        "        x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_17')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 18\n",
        "        x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_18')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 19\n",
        "        x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_19')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 20\n",
        "        x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_20')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        # Layer 21\n",
        "        skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "        skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "        skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "        skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "        x = concatenate([skip_connection, x])\n",
        "\n",
        "        # Layer 22\n",
        "        x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_22')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        self.feature_extractor = Model(input_image, x)\n",
        "        self.feature_extractor.load_weights(FULL_YOLO_BACKEND_PATH,by_name=True)\n",
        "\n",
        "    def normalize(self, image):\n",
        "        return image / 255.\n",
        "\n",
        "class TinyYoloFeature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        input_image = Input(shape=(input_size, input_size, 3))\n",
        "\n",
        "        # Layer 1\n",
        "        x = Conv2D(16, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
        "        x = BatchNormalization(name='norm_1')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 2 - 5\n",
        "        for i in range(0,4):\n",
        "            x = Conv2D(32*(2**i), (3,3), strides=(1,1), padding='same', name='conv_' + str(i+2), use_bias=False)(x)\n",
        "            x = BatchNormalization(name='norm_' + str(i+2))(x)\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Layer 6\n",
        "        x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
        "        x = BatchNormalization(name='norm_6')(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='same')(x)\n",
        "\n",
        "        # Layer 7 - 8\n",
        "        for i in range(0,2):\n",
        "            x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_' + str(i+7), use_bias=False)(x)\n",
        "            x = BatchNormalization(name='norm_' + str(i+7))(x)\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        self.feature_extractor = Model(input_image, x)\n",
        "        self.feature_extractor.load_weights(TINY_YOLO_BACKEND_PATH)\n",
        "\n",
        "    def normalize(self, image):\n",
        "        return image / 255.\n",
        "\n",
        "class MobileNetFeature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        input_image = Input(shape=(input_size, input_size, 3))\n",
        "\n",
        "        mobilenet = MobileNet(input_shape=(224,224,3), include_top=False)\n",
        "        mobilenet.load_weights(MOBILENET_BACKEND_PATH)\n",
        "\n",
        "        x = mobilenet(input_image)\n",
        "\n",
        "        self.feature_extractor = Model(input_image, x)\n",
        "\n",
        "    def normalize(self, image):\n",
        "        image = image / 255.\n",
        "        image = image - 0.5\n",
        "        image = image * 2.\n",
        "\n",
        "        return image\n",
        "\n",
        "class SqueezeNetFeature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "\n",
        "        # define some auxiliary variables and the fire module\n",
        "        sq1x1  = \"squeeze1x1\"\n",
        "        exp1x1 = \"expand1x1\"\n",
        "        exp3x3 = \"expand3x3\"\n",
        "        relu   = \"relu_\"\n",
        "\n",
        "        def fire_module(x, fire_id, squeeze=16, expand=64):\n",
        "            s_id = 'fire' + str(fire_id) + '/'\n",
        "\n",
        "            x     = Conv2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
        "            x     = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
        "\n",
        "            left  = Conv2D(expand,  (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
        "            left  = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
        "\n",
        "            right = Conv2D(expand,  (3, 3), padding='same',  name=s_id + exp3x3)(x)\n",
        "            right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
        "\n",
        "            x = concatenate([left, right], axis=3, name=s_id + 'concat')\n",
        "\n",
        "            return x\n",
        "\n",
        "        # define the model of SqueezeNet\n",
        "        input_image = Input(shape=(input_size, input_size, 3))\n",
        "\n",
        "        x = Conv2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(input_image)\n",
        "        x = Activation('relu', name='relu_conv1')(x)\n",
        "        x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "        x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "        x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "        x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
        "\n",
        "        x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "        x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "        x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
        "\n",
        "        x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "        x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "        x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "        x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "\n",
        "        self.feature_extractor = Model(input_image, x)\n",
        "        self.feature_extractor.load_weights(SQUEEZENET_BACKEND_PATH)\n",
        "\n",
        "    def normalize(self, image):\n",
        "        image = image[..., ::-1]\n",
        "        image = image.astype('float')\n",
        "\n",
        "        image[..., 0] -= 103.939\n",
        "        image[..., 1] -= 116.779\n",
        "        image[..., 2] -= 123.68\n",
        "\n",
        "        return image\n",
        "\n",
        "class Inception3Feature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        input_image = Input(shape=(input_size, input_size, 3))\n",
        "\n",
        "        inception = InceptionV3(input_shape=(input_size,input_size,3), include_top=False)\n",
        "        inception.load_weights(INCEPTION3_BACKEND_PATH)\n",
        "\n",
        "        x = inception(input_image)\n",
        "\n",
        "        self.feature_extractor = Model(input_image, x)\n",
        "\n",
        "    def normalize(self, image):\n",
        "        image = image / 255.\n",
        "        image = image - 0.5\n",
        "        image = image * 2.\n",
        "\n",
        "        return image\n",
        "\n",
        "class VGG16Feature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        vgg16 = VGG16(input_shape=(input_size, input_size, 3), include_top=False)\n",
        "        #vgg16.load_weights(VGG16_BACKEND_PATH)\n",
        "\n",
        "        self.feature_extractor = vgg16\n",
        "\n",
        "    def normalize(self, image):\n",
        "        image = image[..., ::-1]\n",
        "        image = image.astype('float')\n",
        "\n",
        "        image[..., 0] -= 103.939\n",
        "        image[..., 1] -= 116.779\n",
        "        image[..., 2] -= 123.68\n",
        "\n",
        "        return image\n",
        "\n",
        "class ResNet50Feature(BaseFeatureExtractor):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        resnet50 = ResNet50(input_shape=(input_size, input_size, 3), include_top=False)\n",
        "        resnet50.layers.pop() # remove the average pooling layer\n",
        "        #resnet50.load_weights(RESNET50_BACKEND_PATH)\n",
        "\n",
        "        self.feature_extractor = Model(resnet50.layers[0].input, resnet50.layers[-1].output)\n",
        "\n",
        "    def normalize(self, image):\n",
        "        image = image[..., ::-1]\n",
        "        image = image.astype('float')\n",
        "\n",
        "        image[..., 0] -= 103.939\n",
        "        image[..., 1] -= 116.779\n",
        "        image[..., 2] -= 123.68\n",
        "\n",
        "        return image\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFvHCqiHj9iR"
      },
      "source": [
        "help(applications)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGRugrAUnNrv"
      },
      "source": [
        "###################\n",
        "#Part A/2. utils.py from https://github.com/jhasuman/potholes-detection\n",
        "###################\n",
        "import numpy as np\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import cv2\n",
        "\n",
        "class BoundBox:\n",
        "    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n",
        "        self.xmin = xmin\n",
        "        self.ymin = ymin\n",
        "        self.xmax = xmax\n",
        "        self.ymax = ymax\n",
        "\n",
        "        self.c     = c\n",
        "        self.classes = classes\n",
        "\n",
        "        self.label = -1\n",
        "        self.score = -1\n",
        "\n",
        "    def get_label(self):\n",
        "        if self.label == -1:\n",
        "            self.label = np.argmax(self.classes)\n",
        "\n",
        "        return self.label\n",
        "\n",
        "    def get_score(self):\n",
        "        if self.score == -1:\n",
        "            self.score = self.classes[self.get_label()]\n",
        "\n",
        "        return self.score\n",
        "\n",
        "class WeightReader:\n",
        "    def __init__(self, weight_file):\n",
        "        self.offset = 4\n",
        "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
        "\n",
        "    def read_bytes(self, size):\n",
        "        self.offset = self.offset + size\n",
        "        return self.all_weights[self.offset-size:self.offset]\n",
        "\n",
        "    def reset(self):\n",
        "        self.offset = 4\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\n",
        "    intersect = intersect_w * intersect_h\n",
        "\n",
        "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\n",
        "    union = w1*h1 + w2*h2 - intersect\n",
        "\n",
        "    return float(intersect) / union\n",
        "\n",
        "def draw_boxes(image, boxes, labels):\n",
        "    image_h, image_w, _ = image.shape\n",
        "\n",
        "    for box in boxes:\n",
        "        xmin = int(box.xmin*image_w)\n",
        "        ymin = int(box.ymin*image_h)\n",
        "        xmax = int(box.xmax*image_w)\n",
        "        ymax = int(box.ymax*image_h)\n",
        "\n",
        "        cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,255,0), 3)\n",
        "        cv2.putText(image,\n",
        "                    labels[box.get_label()] + ' ' + str(box.get_score()),\n",
        "                    (xmin, ymin - 13),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1e-3 * image_h,\n",
        "                    (0,255,0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
        "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    # decode the output by the network\n",
        "    netout[..., 4]  = _sigmoid(netout[..., 4])\n",
        "    netout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
        "    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
        "\n",
        "    for row in range(grid_h):\n",
        "        for col in range(grid_w):\n",
        "            for b in range(nb_box):\n",
        "                # from 4th element onwards are confidence and class classes\n",
        "                classes = netout[row,col,b,5:]\n",
        "\n",
        "                if np.sum(classes) > 0:\n",
        "                    # first 4 elements are x, y, w, and h\n",
        "                    x, y, w, h = netout[row,col,b,:4]\n",
        "\n",
        "                    x = (col + _sigmoid(x)) / grid_w # center position, unit: image width\n",
        "                    y = (row + _sigmoid(y)) / grid_h # center position, unit: image height\n",
        "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
        "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
        "                    confidence = netout[row,col,b,4]\n",
        "\n",
        "                    box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, confidence, classes)\n",
        "\n",
        "                    boxes.append(box)\n",
        "\n",
        "    # suppress non-maximal boxes\n",
        "    for c in range(nb_class):\n",
        "        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
        "\n",
        "        for i in range(len(sorted_indices)):\n",
        "            index_i = sorted_indices[i]\n",
        "\n",
        "            if boxes[index_i].classes[c] == 0:\n",
        "                continue\n",
        "            else:\n",
        "                for j in range(i+1, len(sorted_indices)):\n",
        "                    index_j = sorted_indices[j]\n",
        "\n",
        "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
        "                        boxes[index_j].classes[c] = 0\n",
        "\n",
        "    # remove the boxes which are less likely than a obj_threshold\n",
        "    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
        "\n",
        "    return boxes\n",
        "\n",
        "def compute_overlap(a, b):\n",
        "    \"\"\"\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "    Parameters\n",
        "    ----------\n",
        "    a: (N, 4) ndarray of float\n",
        "    b: (K, 4) ndarray of float\n",
        "    Returns\n",
        "    -------\n",
        "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
        "    \"\"\"\n",
        "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
        "\n",
        "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
        "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
        "\n",
        "    iw = np.maximum(iw, 0)\n",
        "    ih = np.maximum(ih, 0)\n",
        "\n",
        "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
        "\n",
        "    ua = np.maximum(ua, np.finfo(float).eps)\n",
        "\n",
        "    intersection = iw * ih\n",
        "\n",
        "    return intersection / ua\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.], recall, [1.]))\n",
        "    mpre = np.concatenate(([0.], precision, [0.]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "    x1, x2 = interval_a\n",
        "    x3, x4 = interval_b\n",
        "\n",
        "    if x3 < x1:\n",
        "        if x4 < x1:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x1\n",
        "    else:\n",
        "        if x2 < x3:\n",
        "             return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x3\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        "\n",
        "def _softmax(x, axis=-1, t=-100.):\n",
        "    x = x - np.max(x)\n",
        "\n",
        "    if np.min(x) < t:\n",
        "        x = x/np.min(x)*t\n",
        "\n",
        "    e_x = np.exp(x)\n",
        "\n",
        "    return e_x / e_x.sum(axis, keepdims=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgZDSUYnDQC"
      },
      "source": [
        "###################\n",
        "#Part A/3. frontend.py from https://github.com/jhasuman/potholes-detection\n",
        "###################\n",
        "from keras.models import Model\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "class YOLO(object):\n",
        "    def __init__(self, backend,\n",
        "                       input_size,\n",
        "                       labels,\n",
        "                       max_box_per_image,\n",
        "                       anchors):\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.labels   = list(labels)\n",
        "        self.nb_class = len(self.labels)\n",
        "        self.nb_box   = len(anchors)//2\n",
        "        self.class_wt = np.ones(self.nb_class, dtype='float32')\n",
        "        self.anchors  = anchors\n",
        "\n",
        "        self.max_box_per_image = max_box_per_image\n",
        "\n",
        "        ##########################\n",
        "        # Make the model\n",
        "        ##########################\n",
        "\n",
        "        # make the feature extractor layers\n",
        "        input_image     = Input(shape=(self.input_size, self.input_size, 3))\n",
        "        self.true_boxes = Input(shape=(1, 1, 1, max_box_per_image , 4))\n",
        "\n",
        "        if backend == 'Inception3':\n",
        "            self.feature_extractor = Inception3Feature(self.input_size)\n",
        "        elif backend == 'SqueezeNet':\n",
        "            self.feature_extractor = SqueezeNetFeature(self.input_size)\n",
        "        elif backend == 'MobileNet':\n",
        "            self.feature_extractor = MobileNetFeature(self.input_size)\n",
        "        elif backend == 'Full Yolo':\n",
        "            self.feature_extractor = FullYoloFeature(self.input_size)\n",
        "        elif backend == 'Tiny Yolo':\n",
        "            self.feature_extractor = TinyYoloFeature(self.input_size)\n",
        "        elif backend == 'VGG16':\n",
        "            self.feature_extractor = VGG16Feature(self.input_size)\n",
        "        elif backend == 'ResNet50':\n",
        "            self.feature_extractor = ResNet50Feature(self.input_size)\n",
        "        else:\n",
        "            raise Exception('Architecture not supported! Only support Full Yolo, Tiny Yolo, MobileNet, SqueezeNet, VGG16, ResNet50, and Inception3 at the moment!')\n",
        "\n",
        "        print(self.feature_extractor.get_output_shape())\n",
        "        self.grid_h, self.grid_w = self.feature_extractor.get_output_shape()\n",
        "        features = self.feature_extractor.extract(input_image)\n",
        "\n",
        "        # make the object detection layer\n",
        "        output = Conv2D(self.nb_box * (4 + 1 + self.nb_class),\n",
        "                        (1,1), strides=(1,1),\n",
        "                        padding='same',\n",
        "                        name='DetectionLayer',\n",
        "                        kernel_initializer='lecun_normal')(features)\n",
        "        output = Reshape((self.grid_h, self.grid_w, self.nb_box, 4 + 1 + self.nb_class))(output)\n",
        "        output = Lambda(lambda args: args[0])([output, self.true_boxes])\n",
        "\n",
        "        self.model = Model([input_image, self.true_boxes], output)\n",
        "\n",
        "\n",
        "        # initialize the weights of the detection layer\n",
        "        layer = self.model.layers[-4]\n",
        "        weights = layer.get_weights()\n",
        "\n",
        "        new_kernel = np.random.normal(size=weights[0].shape)/(self.grid_h*self.grid_w)\n",
        "        new_bias   = np.random.normal(size=weights[1].shape)/(self.grid_h*self.grid_w)\n",
        "\n",
        "        layer.set_weights([new_kernel, new_bias])\n",
        "\n",
        "        # print a summary of the whole model\n",
        "        self.model.summary()\n",
        "\n",
        "    def custom_loss(self, y_true, y_pred):\n",
        "        mask_shape = tf.shape(y_true)[:4]\n",
        "\n",
        "        cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(self.grid_w), [self.grid_h]), (1, self.grid_h, self.grid_w, 1, 1)))\n",
        "        cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
        "\n",
        "        cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [self.batch_size, 1, 1, self.nb_box, 1])\n",
        "\n",
        "        coord_mask = tf.zeros(mask_shape)\n",
        "        conf_mask  = tf.zeros(mask_shape)\n",
        "        class_mask = tf.zeros(mask_shape)\n",
        "\n",
        "        seen = tf.Variable(0.)\n",
        "        total_recall = tf.Variable(0.)\n",
        "\n",
        "        \"\"\"\n",
        "        Adjust prediction\n",
        "        \"\"\"\n",
        "        ### adjust x and y\n",
        "        pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
        "\n",
        "        ### adjust w and h\n",
        "        pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(self.anchors, [1,1,1,self.nb_box,2])\n",
        "\n",
        "        ### adjust confidence\n",
        "        pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
        "\n",
        "        ### adjust class probabilities\n",
        "        pred_box_class = y_pred[..., 5:]\n",
        "\n",
        "        \"\"\"\n",
        "        Adjust ground truth\n",
        "        \"\"\"\n",
        "        ### adjust x and y\n",
        "        true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
        "\n",
        "        ### adjust w and h\n",
        "        true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
        "\n",
        "        ### adjust confidence\n",
        "        true_wh_half = true_box_wh / 2.\n",
        "        true_mins    = true_box_xy - true_wh_half\n",
        "        true_maxes   = true_box_xy + true_wh_half\n",
        "\n",
        "        pred_wh_half = pred_box_wh / 2.\n",
        "        pred_mins    = pred_box_xy - pred_wh_half\n",
        "        pred_maxes   = pred_box_xy + pred_wh_half\n",
        "\n",
        "        intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\n",
        "        true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
        "        pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
        "\n",
        "        union_areas = pred_areas + true_areas - intersect_areas\n",
        "        iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "        true_box_conf = iou_scores * y_true[..., 4]\n",
        "\n",
        "        ### adjust class probabilities\n",
        "        true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
        "\n",
        "        \"\"\"\n",
        "        Determine the masks\n",
        "        \"\"\"\n",
        "        ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
        "        coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * self.coord_scale\n",
        "\n",
        "        ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
        "        # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
        "        true_xy = self.true_boxes[..., 0:2]\n",
        "        true_wh = self.true_boxes[..., 2:4]\n",
        "\n",
        "        true_wh_half = true_wh / 2.\n",
        "        true_mins    = true_xy - true_wh_half\n",
        "        true_maxes   = true_xy + true_wh_half\n",
        "\n",
        "        pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
        "        pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
        "\n",
        "        pred_wh_half = pred_wh / 2.\n",
        "        pred_mins    = pred_xy - pred_wh_half\n",
        "        pred_maxes   = pred_xy + pred_wh_half\n",
        "\n",
        "        intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\n",
        "        true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
        "        pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
        "\n",
        "        union_areas = pred_areas + true_areas - intersect_areas\n",
        "        iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "        best_ious = tf.reduce_max(iou_scores, axis=4)\n",
        "        conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * self.no_object_scale\n",
        "\n",
        "        # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
        "        conf_mask = conf_mask + y_true[..., 4] * self.object_scale\n",
        "\n",
        "        ### class mask: simply the position of the ground truth boxes (the predictors)\n",
        "        class_mask = y_true[..., 4] * tf.gather(self.class_wt, true_box_class) * self.class_scale\n",
        "\n",
        "        \"\"\"\n",
        "        Warm-up training\n",
        "        \"\"\"\n",
        "        no_boxes_mask = tf.to_float(coord_mask < self.coord_scale/2.)\n",
        "        seen = tf.assign_add(seen, 1.)\n",
        "\n",
        "        true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, self.warmup_batches+1),\n",
        "                              lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask,\n",
        "                                       true_box_wh + tf.ones_like(true_box_wh) * \\\n",
        "                                       np.reshape(self.anchors, [1,1,1,self.nb_box,2]) * \\\n",
        "                                       no_boxes_mask,\n",
        "                                       tf.ones_like(coord_mask)],\n",
        "                              lambda: [true_box_xy,\n",
        "                                       true_box_wh,\n",
        "                                       coord_mask])\n",
        "\n",
        "        \"\"\"\n",
        "        Finalize the loss\n",
        "        \"\"\"\n",
        "        nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
        "        nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
        "        nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
        "\n",
        "        loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "        loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "        loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
        "        loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
        "        loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
        "\n",
        "        loss = tf.cond(tf.less(seen, self.warmup_batches+1),\n",
        "                      lambda: loss_xy + loss_wh + loss_conf + loss_class + 10,\n",
        "                      lambda: loss_xy + loss_wh + loss_conf + loss_class)\n",
        "\n",
        "        if self.debug:\n",
        "            nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
        "            nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
        "\n",
        "            current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
        "            total_recall = tf.assign_add(total_recall, current_recall)\n",
        "\n",
        "            loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
        "            loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
        "            loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
        "            loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
        "            loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
        "            loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
        "            loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def load_weights(self, weight_path):\n",
        "        self.model.load_weights(weight_path)\n",
        "\n",
        "    def train(self, train_imgs,     # the list of images to train the model\n",
        "                    valid_imgs,     # the list of images used to validate the model\n",
        "                    train_times,    # the number of time to repeat the training set, often used for small datasets\n",
        "                    valid_times,    # the number of times to repeat the validation set, often used for small datasets\n",
        "                    nb_epochs,      # number of epoches\n",
        "                    learning_rate,  # the learning rate\n",
        "                    batch_size,     # the size of the batch\n",
        "                    warmup_epochs,  # number of initial batches to let the model familiarize with the new dataset\n",
        "                    object_scale,\n",
        "                    no_object_scale,\n",
        "                    coord_scale,\n",
        "                    class_scale,\n",
        "                    saved_weights_name='best_weights.h5',\n",
        "                    debug=False):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.object_scale    = object_scale\n",
        "        self.no_object_scale = no_object_scale\n",
        "        self.coord_scale     = coord_scale\n",
        "        self.class_scale     = class_scale\n",
        "\n",
        "        self.debug = debug\n",
        "\n",
        "        ############################################\n",
        "        # Make train and validation generators\n",
        "        ############################################\n",
        "\n",
        "        generator_config = {\n",
        "            'IMAGE_H'         : self.input_size,\n",
        "            'IMAGE_W'         : self.input_size,\n",
        "            'GRID_H'          : self.grid_h,\n",
        "            'GRID_W'          : self.grid_w,\n",
        "            'BOX'             : self.nb_box,\n",
        "            'LABELS'          : self.labels,\n",
        "            'CLASS'           : len(self.labels),\n",
        "            'ANCHORS'         : self.anchors,\n",
        "            'BATCH_SIZE'      : self.batch_size,\n",
        "            'TRUE_BOX_BUFFER' : self.max_box_per_image,\n",
        "        }\n",
        "\n",
        "        train_generator = BatchGenerator(train_imgs,\n",
        "                                     generator_config,\n",
        "                                     norm=self.feature_extractor.normalize)\n",
        "        valid_generator = BatchGenerator(valid_imgs,\n",
        "                                     generator_config,\n",
        "                                     norm=self.feature_extractor.normalize,\n",
        "                                     jitter=False)\n",
        "\n",
        "        self.warmup_batches  = warmup_epochs * (train_times*len(train_generator) + valid_times*len(valid_generator))\n",
        "\n",
        "        ############################################\n",
        "        # Compile the model\n",
        "        ############################################\n",
        "\n",
        "        optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "        self.model.compile(loss=self.custom_loss, optimizer=optimizer)\n",
        "\n",
        "        ############################################\n",
        "        # Make a few callbacks\n",
        "        ############################################\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss',\n",
        "                           min_delta=0.001,\n",
        "                           patience=3,\n",
        "                           mode='min',\n",
        "                           verbose=1)\n",
        "        checkpoint = ModelCheckpoint(saved_weights_name,\n",
        "                                     monitor='val_loss',\n",
        "                                     verbose=1,\n",
        "                                     save_best_only=True,\n",
        "                                     mode='min',\n",
        "                                     period=1)\n",
        "        tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/'),\n",
        "                                  histogram_freq=0,\n",
        "                                  #write_batch_performance=True,\n",
        "                                  write_graph=True,\n",
        "                                  write_images=False)\n",
        "\n",
        "        ############################################\n",
        "        # Start the training process\n",
        "        ############################################\n",
        "\n",
        "        self.model.fit_generator(generator        = train_generator,\n",
        "                                 steps_per_epoch  = len(train_generator) * train_times,\n",
        "                                 epochs           = warmup_epochs + nb_epochs,\n",
        "                                 verbose          = 2 if debug else 1,\n",
        "                                 validation_data  = valid_generator,\n",
        "                                 validation_steps = len(valid_generator) * valid_times,\n",
        "                                 callbacks        = [early_stop, checkpoint, tensorboard],\n",
        "                                 workers          = 3,\n",
        "                                 max_queue_size   = 8)\n",
        "\n",
        "        ############################################\n",
        "        # Compute mAP on the validation set\n",
        "        ############################################\n",
        "        average_precisions = self.evaluate(valid_generator)\n",
        "\n",
        "        # print evaluation\n",
        "        for label, average_precision in average_precisions.items():\n",
        "            print(self.labels[label], '{:.4f}'.format(average_precision))\n",
        "        print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))\n",
        "\n",
        "    def evaluate(self,\n",
        "                 generator,\n",
        "                 iou_threshold=0.3,\n",
        "                 score_threshold=0.3,\n",
        "                 max_detections=100,\n",
        "                 save_path=None):\n",
        "        \"\"\" Evaluate a given dataset using a given model.\n",
        "        code originally from https://github.com/fizyr/keras-retinanet\n",
        "\n",
        "        # Arguments\n",
        "            generator       : The generator that represents the dataset to evaluate.\n",
        "            model           : The model to evaluate.\n",
        "            iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
        "            score_threshold : The score confidence threshold to use for detections.\n",
        "            max_detections  : The maximum number of detections to use per image.\n",
        "            save_path       : The path to save images with visualized detections to.\n",
        "        # Returns\n",
        "            A dict mapping class names to mAP scores.\n",
        "        \"\"\"\n",
        "        # gather all detections and annotations\n",
        "        all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
        "        all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
        "\n",
        "        for i in range(generator.size()):\n",
        "            raw_image = generator.load_image(i)\n",
        "            raw_height, raw_width, raw_channels = raw_image.shape\n",
        "\n",
        "            # make the boxes and the labels\n",
        "            pred_boxes  = self.predict(raw_image)\n",
        "\n",
        "\n",
        "            score = np.array([box.score for box in pred_boxes])\n",
        "            pred_labels = np.array([box.label for box in pred_boxes])\n",
        "\n",
        "            if len(pred_boxes) > 0:\n",
        "                pred_boxes = np.array([[box.xmin*raw_width, box.ymin*raw_height, box.xmax*raw_width, box.ymax*raw_height, box.score] for box in pred_boxes])\n",
        "            else:\n",
        "                pred_boxes = np.array([[]])\n",
        "\n",
        "            # sort the boxes and the labels according to scores\n",
        "            score_sort = np.argsort(-score)\n",
        "            pred_labels = pred_labels[score_sort]\n",
        "            pred_boxes  = pred_boxes[score_sort]\n",
        "\n",
        "            # copy detections to all_detections\n",
        "            for label in range(generator.num_classes()):\n",
        "                all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
        "\n",
        "            annotations = generator.load_annotation(i)\n",
        "\n",
        "            # copy detections to all_annotations\n",
        "            for label in range(generator.num_classes()):\n",
        "                all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
        "\n",
        "        # compute mAP by comparing all detections and all annotations\n",
        "        average_precisions = {}\n",
        "\n",
        "        for label in range(generator.num_classes()):\n",
        "            false_positives = np.zeros((0,))\n",
        "            true_positives  = np.zeros((0,))\n",
        "            scores          = np.zeros((0,))\n",
        "            num_annotations = 0.0\n",
        "\n",
        "            for i in range(generator.size()):\n",
        "                detections           = all_detections[i][label]\n",
        "                annotations          = all_annotations[i][label]\n",
        "                num_annotations     += annotations.shape[0]\n",
        "                detected_annotations = []\n",
        "\n",
        "                for d in detections:\n",
        "                    scores = np.append(scores, d[4])\n",
        "\n",
        "                    if annotations.shape[0] == 0:\n",
        "                        false_positives = np.append(false_positives, 1)\n",
        "                        true_positives  = np.append(true_positives, 0)\n",
        "                        continue\n",
        "\n",
        "                    overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
        "                    assigned_annotation = np.argmax(overlaps, axis=1)\n",
        "                    max_overlap         = overlaps[0, assigned_annotation]\n",
        "\n",
        "                    if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
        "                        false_positives = np.append(false_positives, 0)\n",
        "                        true_positives  = np.append(true_positives, 1)\n",
        "                        detected_annotations.append(assigned_annotation)\n",
        "                    else:\n",
        "                        false_positives = np.append(false_positives, 1)\n",
        "                        true_positives  = np.append(true_positives, 0)\n",
        "\n",
        "            # no annotations -> AP for this class is 0 (is this correct?)\n",
        "            if num_annotations == 0:\n",
        "                average_precisions[label] = 0\n",
        "                continue\n",
        "\n",
        "            # sort by score\n",
        "            indices         = np.argsort(-scores)\n",
        "            false_positives = false_positives[indices]\n",
        "            true_positives  = true_positives[indices]\n",
        "\n",
        "            # compute false positives and true positives\n",
        "            false_positives = np.cumsum(false_positives)\n",
        "            true_positives  = np.cumsum(true_positives)\n",
        "\n",
        "            # compute recall and precision\n",
        "            recall    = true_positives / num_annotations\n",
        "            precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
        "\n",
        "            # compute average precision\n",
        "            average_precision  = compute_ap(recall, precision)\n",
        "            average_precisions[label] = average_precision\n",
        "\n",
        "        return average_precisions\n",
        "\n",
        "    def predict(self, image):\n",
        "        image_h, image_w, _ = image.shape\n",
        "        image = cv2.resize(image, (self.input_size, self.input_size))\n",
        "        image = self.feature_extractor.normalize(image)\n",
        "\n",
        "        input_image = image[:,:,::-1]\n",
        "        input_image = np.expand_dims(input_image, 0)\n",
        "        dummy_array = np.zeros((1,1,1,1,self.max_box_per_image,4))\n",
        "\n",
        "        netout = self.model.predict([input_image, dummy_array])[0]\n",
        "        boxes  = decode_netout(netout, self.anchors, self.nb_class)\n",
        "\n",
        "        return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQvhbRQ_oF84",
        "outputId": "03b4bb37-f05d-437c-9b9b-e95121ef75ef"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  hello = tf.constant('hello world')\n",
        "  print(sess.run(hello))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'hello world'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF6gOCnisfzx",
        "outputId": "332ec392-c942-416f-9283-7e593312128c"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgVxdMRCmFcn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "1d3c9321-4644-40b7-ee59-a3d6a914eb26"
      },
      "source": [
        "###################\n",
        "#Part B/1. Frozen graph (freeze tensor flow model)\n",
        "###################\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "\n",
        "#Jordan_note: Getting the failedpreconditionerror, I followed user \"user3144836\"'s response, where I decided to encapsulate the entire code block nder the with routine he suggested.\n",
        "#this removed error.\n",
        "#https://stackoverflow.com/a/41607207/7183973\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "  def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "      \"\"\"\n",
        "      Freezes the state of a session into a pruned computation graph.\n",
        "\n",
        "      Creates a new computation graph where variable nodes are replaced by\n",
        "      constants taking their current value in the session. The new graph will be\n",
        "      pruned so subgraphs that are not necessary to compute the requested\n",
        "      outputs are removed.\n",
        "      @param session The TensorFlow session to be frozen.\n",
        "      @param keep_var_names A list of variable names that should not be frozen,\n",
        "                            or None to freeze all the variables in the graph.\n",
        "      @param output_names Names of the relevant graph outputs.\n",
        "      @param clear_devices Remove the device directives from the graph for better portability.\n",
        "      @return The frozen graph definition.\n",
        "      \"\"\"\n",
        "      graph = session.compat.v1.graph\n",
        "      with graph.as_default():\n",
        "          freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))\n",
        "          output_names = output_names or []\n",
        "          output_names += [v.op.name for v in tf.compat.v1.global_variables()]\n",
        "          input_graph_def = graph.as_graph_def()\n",
        "          if clear_devices:\n",
        "              for node in input_graph_def.node:\n",
        "                  node.device = ''\n",
        "          frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "              session, input_graph_def, output_names, freeze_var_names)\n",
        "          return frozen_graph\n",
        "\n",
        "  #load trained weights and config file from google drive account\n",
        "  jordan_drive_trained_weights_id = '10Ymwx7BCYye6hZHewS1FctgENEk3R7V1';\n",
        "\n",
        "  file_obj_trained_weights = drive.CreateFile({'id': jordan_drive_trained_weights_id})\n",
        "\n",
        "  file_obj_trained_weights.GetContentFile ( \"trained_wts.h5\" )\n",
        "\n",
        "\n",
        "  jordan_drive_trained_weights_CONFIG_id = \"1Qny9it02xW-v2Kbys3tgPLaMNftAibCC\"\n",
        "\n",
        "  file_obj_trained_weights_config = drive.CreateFile({'id': jordan_drive_trained_weights_CONFIG_id})\n",
        "\n",
        "  file_obj_trained_weights_config.GetContentFile ( \"config.json\" )\n",
        "\n",
        "\n",
        "  config_path  = 'config.json'\n",
        "  weights_path = 'trained_wts.h5'\n",
        "\n",
        "  with open(config_path) as config_buffer:\n",
        "      config = json.load(config_buffer)\n",
        "\n",
        "  yolo = YOLO(backend = config['model']['backend'], input_size = config['model']['input_size'], labels = config['model']['labels'], max_box_per_image= config['model']['max_box_per_image'], anchors = config['model']['anchors'] )\n",
        "\n",
        "\n",
        "  ###############################\n",
        "  #   Load trained weights\n",
        "  ###############################\n",
        "\n",
        "  ## dir(model.model) reveals native keras model attributes. Jordan tried .model by curiousity :)\n",
        "\n",
        "  yolo.load_weights(weights_path)\n",
        "\n",
        "  model = yolo.model\n",
        "\n",
        "  input_names = [i.op.name for i in model.inputs]\n",
        "  output_names_ = [o.op.name for o in model.outputs]\n",
        "\n",
        "  # inputs:  ['input_1', 'input_2']\n",
        "  print('inputs: ', input_names)\n",
        "\n",
        "  # outputs: ['lambda_2/Identity']\n",
        "  print('outputs: ', output_names_)\n",
        "\n",
        "  frozen_graph = freeze_session(tf.compat.v1.keras.backend.get_session(), output_names=output_names_)\n",
        "  print (\"frozen_graph composed successfully.\")\n",
        "\n",
        "  #tf.io.write_graph(frozen_graph, './', 'pothole_detector.pbtxt', as_text=True)\n",
        "  #tf.io.write_graph(frozen_graph, './', 'pothole_detector.pb', as_text=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5c4ccaf6207d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0myolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_box_per_image\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_box_per_image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anchors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2d3c8e6382dc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, input_size, labels, max_box_per_image, anchors)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMobileNetFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Full Yolo'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullYoloFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Tiny Yolo'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyYoloFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-bed9f7e4a937>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mskip_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'norm_21'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mskip_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mskip_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_to_depth_x2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    763\u001b[0m               with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m    764\u001b[0m                   self._compute_dtype_object):\n\u001b[0;32m--> 765\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreated_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-bed9f7e4a937>\u001b[0m in \u001b[0;36mspace_to_depth_x2\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mspace_to_depth_x2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace_to_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'space_to_depth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWygvIyctpeI"
      },
      "source": [
        "###################\n",
        "#Part B/2a. Convert frozen graph 32bit (fp32) to tensorrt graph 16 bit (fp16)\n",
        "###################\n",
        "\n",
        "import tensorflow.contrib.tensorrt as trt\n",
        "from tensorflow.python.framework import graph_io\n",
        "\n",
        "trt_graph = trt.create_inference_graph(\n",
        "    input_graph_def=frozen_graph,\n",
        "    outputs=output_names_,\n",
        "    max_batch_size=1,\n",
        "    max_workspace_size_bytes=1 << 25,\n",
        "    precision_mode='FP16',\n",
        "    minimum_segment_size=50\n",
        ")\n",
        "\n",
        "graph_io.write_graph(trt_graph, \"\",\n",
        "                     \"trt_pothole_graph.pb\", as_text=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGqN3UXquW-m"
      },
      "source": [
        "#Part B/2b/Alternative, quickly reference trt graph stored in my google drive. (Instead of regenerating the same trt graph in colab)\n",
        "#This is a file in Jordan's google drive.\n",
        "trt_graph_jordan_drive_id = '1b9XgpXeWBay6GE2bnLSqlLSXDEFfUCZd';\n",
        "trt_graph_jordan_drive_file = drive.CreateFile({'id': trt_graph_jordan_drive_id})\n",
        "trt_graph_jordan_drive_file.GetContentFile ( \"trt_pothole_graph.pb\" ) #allows colab to access network sample prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Jx1Yq0uejv"
      },
      "source": [
        "###################\n",
        "#Part B/3a. Load tensor RT fp16 graph\n",
        "###################\n",
        "import tensorflow.compat.v1 as tf #Jordan_note: Resolves  module 'tensorflow' has no attribute 'gfile' error, since Collab seems to have updated these.\n",
        "\n",
        "\n",
        "#jordan_declaration: input and output names taken from frozen graph generation process\n",
        "#this is to avoid re-running frozen graph generation on jetson nano, instead okay to use generatred frozen graph file on storage.\n",
        "input_names =  ['input_1', 'input_2']\n",
        "output_names_ = ['lambda_2/Identity']\n",
        "\n",
        "def get_frozen_graph(graph_file):\n",
        "    \"\"\"Read Frozen Graph file from disk.\"\"\"\n",
        "    with tf.gfile.FastGFile(graph_file, \"rb\") as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    return graph_def\n",
        "\n",
        "\n",
        "#trt_graph = get_frozen_graph('pothole_model_tensor_rt_format/trt_pothole_graph.pb') #reads from colab directory\n",
        "trt_graph = get_frozen_graph('trt_pothole_graph.pb') #reads from google drive\n",
        "\n",
        "# Create session and load graph\n",
        "tf_config = tf.ConfigProto()\n",
        "tf_config.gpu_options.allow_growth = True\n",
        "tf_sess = tf.Session(config=tf_config)\n",
        "tf.import_graph_def(trt_graph, name='')\n",
        "\n",
        "\n",
        "# Get graph input size\n",
        "for node in trt_graph.node:\n",
        "    if 'input_' in node.name:\n",
        "        size = node.attr['shape'].shape\n",
        "        image_size = [size.dim[i].size for i in range(1, 4)]\n",
        "        break\n",
        "print(\"image_size: {}\".format(image_size))\n",
        "\n",
        "\n",
        "# input and output tensor names.\n",
        "input_tensor_name = input_names[0] + \":0\"\n",
        "output_tensor_name = output_names_[0] + \":0\"\n",
        "\n",
        "print(\"input_tensor_name: {}\\noutput_tensor_name: {}\".format(\n",
        "    input_tensor_name, output_tensor_name))\n",
        "\n",
        "with tf.Session() as sess: #jordan_node added these two lines to resolve FailedPreconditionError, that happens in Part B/4 prediciton on runtime.\n",
        "  tf_sess.run(tf.global_variables_initializer())\n",
        "\n",
        "output_tensor = tf_sess.graph.get_tensor_by_name(output_tensor_name)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azhh5OA2vI72"
      },
      "source": [
        "###################\n",
        "#Part B/3b. Ready-made decoder to decode pothole neural network output from 5D array, to simple outcome.\n",
        "#Note by Jordan: I took this from original desktop-version pothole code. I took relevant params to decode_netout from the config file of said repository.\n",
        "#The only param that is dynamic here is the neural network hypothesis/output=\"netout\".\n",
        "###################\n",
        "import numpy as np\n",
        "\n",
        "anchors__=[0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
        "obj_threshold__=0.3\n",
        "nms_threshold__=0.3\n",
        "nb_class__=1 #where 1 is equivalent to the number of labels of this neural network, i.e. only \"pothole\"\n",
        "\n",
        "\n",
        "def decode_hypothesis(netout, anchors=anchors__, nb_class=nb_class__, obj_threshold=obj_threshold__, nms_threshold=nms_threshold__):\n",
        "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    # decode the output by the network\n",
        "    netout[..., 4]  = _sigmoid(netout[..., 4])\n",
        "    netout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
        "    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
        "\n",
        "    for row in range(grid_h):\n",
        "        for col in range(grid_w):\n",
        "            for b in range(nb_box):\n",
        "                # from 4th element onwards are confidence and class classes\n",
        "                classes = netout[row,col,b,5:]\n",
        "\n",
        "                if np.sum(classes) > 0:\n",
        "                    # first 4 elements are x, y, w, and h\n",
        "                    x, y, w, h = netout[row,col,b,:4]\n",
        "\n",
        "                    x = (col + _sigmoid(x)) / grid_w # center position, unit: image width\n",
        "                    y = (row + _sigmoid(y)) / grid_h # center position, unit: image height\n",
        "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
        "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
        "                    confidence = netout[row,col,b,4]\n",
        "\n",
        "                    box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, confidence, classes)\n",
        "\n",
        "                    boxes.append(box)\n",
        "\n",
        "    # suppress non-maximal boxes\n",
        "    for c in range(nb_class):\n",
        "        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
        "\n",
        "        for i in range(len(sorted_indices)):\n",
        "            index_i = sorted_indices[i]\n",
        "\n",
        "            if boxes[index_i].classes[c] == 0:\n",
        "                continue\n",
        "            else:\n",
        "                for j in range(i+1, len(sorted_indices)):\n",
        "                    index_j = sorted_indices[j]\n",
        "\n",
        "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
        "                        boxes[index_j].classes[c] = 0\n",
        "\n",
        "    # remove the boxes which are less likely than a obj_threshold\n",
        "    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
        "\n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrHyjN_Cvk4Z"
      },
      "source": [
        "###################\n",
        "#Written by Jordan Bennett\n",
        "#Part B/4. Make predictions based on FP16 graph.\n",
        "###################\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "#This is a file in Jordan's google drive.\n",
        "jordan_drive_pothole_sample_0_id = '1GIt_daWwyrjV7-pibonwCv-G8Gks3KHR';\n",
        "file_obj_pothole_sample_0 = drive.CreateFile({'id': jordan_drive_pothole_sample_0_id})\n",
        "file_obj_pothole_sample_0.GetContentFile ( \"pothole_sample_0.jpg\" ) #allows colab to access pothole image (1 pothole)\n",
        "\n",
        "#This is a file in Jordan's google drive.\n",
        "jordan_drive_pothole_sample_1_id = '1YVknsYymTdMGLLLdqewOse9LKhieSU6R';\n",
        "file_obj_pothole_sample_1 = drive.CreateFile({'id': jordan_drive_pothole_sample_1_id})\n",
        "file_obj_pothole_sample_1.GetContentFile ( \"pothole_sample_1.jpg\" ) #allows colab to access pothole image (3 potholes)\n",
        "\n",
        "\n",
        "#This is a file in Jordan's google drive.\n",
        "jordan_drive_pothole_sample_2_id = '18EBtBP3b6wfPyk03bGJYN_DHCKbSe6eX';\n",
        "file_obj_pothole_sample_2 = drive.CreateFile({'id': jordan_drive_pothole_sample_2_id})\n",
        "file_obj_pothole_sample_2.GetContentFile ( \"pothole_sample_2.jpg\" ) #allows colab to access pothole image (8 potholes)\n",
        "\n",
        "#This is a file in Jordan's google drive.\n",
        "jordan_drive_pothole_negative_sample_0_id = '1_ik0jpPiulsdkYYRtIbhcRlULVNSUeIe';\n",
        "file_obj_pothole_negative_sample_0 = drive.CreateFile({'id': jordan_drive_pothole_negative_sample_0_id})\n",
        "file_obj_pothole_negative_sample_0.GetContentFile ( \"pothole_negative_sample.jpg\" ) #allows colab to access pothole negative image (no potholes)\n",
        "\n",
        "\n",
        "def getPrediction (image_path__):\n",
        "  img = image.load_img(image_path__, target_size=image_size[:2]) #where image_size[:2] = \"[416,416,3]\", which corresponds somewhat to config[\"input_size\"] in config.json.\n",
        "\n",
        "  x = image.img_to_array(img)/255.0 #CRUCIAL!!!-->jordan_normalize IMAGE_DATA=image.img_to_array(...) as seen in desktop version. Otherwise image data contains large integers, which is not expected by the trained pothole model which expects small normalized floating point values.\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  feed_dict = {\n",
        "      input_tensor_name: x\n",
        "  }\n",
        "\n",
        "  hypothesis = tf_sess.run(output_tensor, feed_dict)\n",
        "\n",
        "  hypothesis = hypothesis.reshape ( 13, 13, 5, 6 ) #jordan_addition: correct network output shape based on observation of desktop output analysis\n",
        "\n",
        "\n",
        "  #jordan_note: The output of the neural network is a bunch of pixels, or bounding boxes. Cardinality of those boxes equals pothole cardinality.\n",
        "  print('Caution!', len(decode_hypothesis(hypothesis)), 'pothole(s) are detected ahead from input image: ', image_path__ )\n",
        "\n",
        "########################################################\n",
        "########################################################\n",
        "#####Test on image sample 0, with 1 potholes\n",
        "getPrediction ('pothole_sample_0.jpg')\n",
        "#####Test on image sample 1, with 3 potholes\n",
        "getPrediction ('pothole_sample_1.jpg')\n",
        "#####Test on image sample 2, with 8 potholes\n",
        "getPrediction ('pothole_sample_2.jpg')\n",
        "#####Test on image sample 3, with 0 potholes\n",
        "getPrediction ('pothole_negative_sample.jpg')\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "###############################################\n",
        "#Runtime cost test. Test speed of prediction on optimized tensor rt graph\n",
        "#this same code is ran in Desktop version, which yielded (except for getPrediction which is swapped with desktop equivalent)\n",
        "print(\"\\n\\n########\\nExecution runtime cost test\")\n",
        "import time\n",
        "times = []\n",
        "for i in range(20):\n",
        "    start_time = time.time()\n",
        "    getPrediction ('pothole_sample_2.jpg')\n",
        "    delta = (time.time() - start_time)\n",
        "    times.append(delta)\n",
        "mean_delta = np.array(times).mean()\n",
        "fps = 1 / mean_delta\n",
        "print('average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}